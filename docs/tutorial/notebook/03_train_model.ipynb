{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cd83b2",
   "metadata": {},
   "source": [
    "## Training a Semantic Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f62dc",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to train a semantic segmentation model using either PyTorch or Tensorflow. \n",
    "\n",
    "## Installing dependencies\n",
    "\n",
    "Before you start, chose either Pyorch or Tensorflow. You cannot chose both. The requirements text files are present in the main directory of Open3D-ML respoitory. Use them to ensure that you have the right dependencies installed in your enviroment. \n",
    "\n",
    "From Open3D-ML main directory,\n",
    "\n",
    "PyTorch users may run:\n",
    "```sh\n",
    "pip install -r requirements-torch-cuda.txt\n",
    "```\n",
    "\n",
    "Tensorflow users may run:\n",
    "```sh\n",
    "pip install -r requirements-tensoflow.txt\n",
    "```\n",
    "\n",
    "Create a folder where your dataset will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af472b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cdb80",
   "metadata": {},
   "source": [
    "### Downloading Toronto3D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15151b22",
   "metadata": {},
   "source": [
    "Open3D-ML provides [scripts](https://github.com/isl-org/Open3D-ML/tree/master/scripts/download_datasets) to download datasets locally. Let us download Toronto3D dataset. (We chose this dataset becuase it is small in size compared to other datasets like SemanticKITTI).\n",
    "\n",
    "Let us use the Toronto3D dataset script to download point clouds. Let use write the downloading script locally first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f719b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/download_toronto3d.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/download_toronto3d.sh\n",
    "#!/bin/bash\n",
    "  \n",
    "if [ \"$#\" -ne 1 ]; then\n",
    "    echo \"Please, provide the base directory to store the dataset.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if ! command -v unzip &> /dev/null\n",
    "then\n",
    "    echo \"Error: unzip could not be found. Please, install it to continue.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "BASE_DIR=\"$1\"/Toronto3D\n",
    "\n",
    "export url=\"https://xx9lca.sn.files.1drv.com/y4mUm9-LiY3vULTW79zlB3xp0wzCPASzteId4wdUZYpzWiw6Jp4IFoIs6ADjLREEk1-IYH8KRGdwFZJrPlIebwytHBYVIidsCwkHhW39aQkh3Vh0OWWMAcLVxYwMTjXwDxHl-CDVDau420OG4iMiTzlsK_RTC_ypo3z-Adf-h0gp2O8j5bOq-2TZd9FD1jPLrkf3759rB-BWDGFskF3AsiB3g\"\n",
    "\n",
    "mkdir -p $BASE_DIR\n",
    "\n",
    "wget -c -N -O $BASE_DIR'/Toronto_3D.zip' $url\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "unzip -j Toronto_3D.zip\n",
    "\n",
    "# cleanup\n",
    "mkdir -p $BASE_DIR/zip_files\n",
    "mv Toronto_3D.zip $BASE_DIR/zip_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610773c7",
   "metadata": {},
   "source": [
    "The bash script takes path to output folder as input where dataset must be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775f199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2022-06-14 22:46:55--  https://xx9lca.sn.files.1drv.com/y4mUm9-LiY3vULTW79zlB3xp0wzCPASzteId4wdUZYpzWiw6Jp4IFoIs6ADjLREEk1-IYH8KRGdwFZJrPlIebwytHBYVIidsCwkHhW39aQkh3Vh0OWWMAcLVxYwMTjXwDxHl-CDVDau420OG4iMiTzlsK_RTC_ypo3z-Adf-h0gp2O8j5bOq-2TZd9FD1jPLrkf3759rB-BWDGFskF3AsiB3g\n",
      "Resolving xx9lca.sn.files.1drv.com (xx9lca.sn.files.1drv.com)... 13.107.42.12\n",
      "Connecting to xx9lca.sn.files.1drv.com (xx9lca.sn.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1143871417 (1.1G) [application/zip]\n",
      "Saving to: ‘data/toronto3d_dataset/Toronto3D/Toronto_3D.zip’\n",
      "\n",
      "data/toronto3d_data 100%[===================>]   1.06G  20.0MB/s    in 67s     \n",
      "\n",
      "2022-06-14 22:48:03 (16.2 MB/s) - ‘data/toronto3d_dataset/Toronto3D/Toronto_3D.zip’ saved [1143871417/1143871417]\n",
      "\n",
      "Archive:  Toronto_3D.zip\n",
      "  inflating: Colors.xml              \n",
      "  inflating: L001.ply                \n",
      "  inflating: L002.ply                \n",
      "  inflating: L003.ply                \n",
      "  inflating: L004.ply                \n",
      "  inflating: Mavericks_classes_9.txt  \n"
     ]
    }
   ],
   "source": [
    "!bash data/download_toronto3d.sh data/toronto3d_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd2f4b",
   "metadata": {},
   "source": [
    "You may see downloaded point cloud files as shown below:\n",
    "\n",
    "```\n",
    "/data/toronto3d\n",
    "└── Toronto3D\n",
    "    ├── Colors.xml\n",
    "    ├── L001.ply\n",
    "    ├── L002.ply\n",
    "    ├── L003.ply\n",
    "    ├── L004.ply\n",
    "    ├── Mavericks_classes_9.txt\n",
    "    └── zip_files\n",
    "        └── Toronto_3D.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec1929",
   "metadata": {},
   "source": [
    "## Limiting memory usage (Tesorflow Users)\n",
    "\n",
    "TensorFlow maps nearly all of GPU memory by default. This may result in out_of_memory error if some of the ops allocate memory independent to tensorflow. You may want to limit memory usage as and when needed by the process. Use following code right after importing tensorflow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b955f11",
   "metadata": {},
   "source": [
    "## Building componets for training\n",
    "\n",
    "First, let us do some basic imports. Import statements are depend on what framework you are using - Pytorch or Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfca8e",
   "metadata": {},
   "source": [
    "Tesorflow users may run:\n",
    "\n",
    "```py\n",
    "import open3d.ml.tf as ml3d\n",
    "from open3d.ml.tf.models import RandLANet\n",
    "from open3d.ml.tf.pipelines import SemanticSegmentation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bd6ab",
   "metadata": {},
   "source": [
    "Pytorch users may run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bcaac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d.ml.torch as ml3d\n",
    "from open3d.ml.torch.models import RandLANet\n",
    "from open3d.ml.torch.pipelines import SemanticSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2861200",
   "metadata": {},
   "source": [
    "The goal was to create `ml3d`, `RandLANet` and `SemanticSegmentation` based on either PyTorch or Tensorflow framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b941e6d",
   "metadata": {},
   "source": [
    "Let us now define a config file with `dataset`, `model` and `pipeline`. Just add dataset path to the config file present in `Open3D-ML/ml3d/configs/randlanet_toronto3d.yml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8319720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/randlanet_toronto3d.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/randlanet_toronto3d.yml\n",
    "dataset:\n",
    "  name: Toronto3D\n",
    "  cache_dir: ./logs/cache\n",
    "  dataset_path: data/toronto3d_dataset/Toronto3D # path/to/your/dataset\n",
    "  class_weights: [41697357, 1745448, 6572572, 19136493, 674897, 897825, 4634634, 374721]\n",
    "  ignored_label_inds:\n",
    "  - 0\n",
    "  num_classes: 8\n",
    "  num_points: 65536\n",
    "  test_files:\n",
    "  - L002.ply\n",
    "  test_result_folder: ./test\n",
    "  train_files:\n",
    "  - L001.ply\n",
    "  - L003.ply\n",
    "  - L004.ply\n",
    "  use_cache: true\n",
    "  val_files:\n",
    "  - L002.ply\n",
    "  steps_per_epoch_train: 100\n",
    "  steps_per_epoch_valid: 10\n",
    "model:\n",
    "  name: RandLANet\n",
    "  batcher: DefaultBatcher\n",
    "  ckpt_path: # path/to/your/checkpoint\n",
    "  num_neighbors: 16\n",
    "  num_layers: 5\n",
    "  num_points: 65536\n",
    "  num_classes: 8\n",
    "  ignored_label_inds: [0]\n",
    "  sub_sampling_ratio: [4, 4, 4, 4, 2]\n",
    "  in_channels: 6\n",
    "  dim_features: 8\n",
    "  dim_output: [16, 64, 128, 256, 512]\n",
    "  grid_size: 0.05\n",
    "  augment:\n",
    "    recenter:\n",
    "      dim: [0, 1, 2]\n",
    "    normalize:\n",
    "      points:\n",
    "        method: linear\n",
    "pipeline:\n",
    "  name: SemanticSegmentation\n",
    "  optimizer:\n",
    "    lr: 0.001\n",
    "  batch_size: 2\n",
    "  main_log_dir: ./logs\n",
    "  max_epoch: 200\n",
    "  save_ckpt_freq: 5\n",
    "  scheduler_gamma: 0.99\n",
    "  test_batch_size: 1\n",
    "  train_sum_dir: train_log\n",
    "  val_batch_size: 2\n",
    "  summary:\n",
    "    record_for: []\n",
    "    max_pts:\n",
    "    use_reference: false\n",
    "    max_outputs: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178391e",
   "metadata": {},
   "source": [
    "Load the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "031d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open3d.ml import utils\n",
    "\n",
    "cfg_file = \"data/randlanet_toronto3d.yml\"\n",
    "cfg = utils.Config.load_from_file(cfg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71ada2",
   "metadata": {},
   "source": [
    "Create dataset object from config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb9aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ml3d.datasets.Toronto3D(**cfg.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576e86b",
   "metadata": {},
   "source": [
    "Create model object from config file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f3d73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandLANet(**cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b07e24",
   "metadata": {},
   "source": [
    "Create a pipeline object from model, dataset and config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a965c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SemanticSegmentation(model=model,\n",
    "                                dataset=dataset,\n",
    "                                **cfg.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e623d",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-14 23:01:41,840 - semantic_segmentation - DEVICE : cpu\n",
      "INFO - 2022-06-14 23:01:41,842 - semantic_segmentation - Logging in file : ./logs/RandLANet_Toronto3D_torch/log_train_2022-06-14_23:01:41.txt\n",
      "INFO - 2022-06-14 23:01:41,844 - toronto3d - Found 3 pointclouds for train\n",
      "preprocess:   0%|                                                | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pipeline.run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f412a7",
   "metadata": {},
   "source": [
    "The training checkpoints are saved in: `pipeline.main_log_dir` (default path is: `'./logs/Model_Dataset/'`). You may use them for testing, inference and re-training purposes.\n",
    "\n",
    "Have a look at training logs and loss curves. Understand if your model is generalizing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a944ff",
   "metadata": {},
   "source": [
    "## Evaluation on holdout split\n",
    "\n",
    "evaluate the trained model on the test split by calling the `run_test()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-15 00:43:22,308 - semantic_segmentation - DEVICE : cpu\n",
      "INFO - 2022-06-15 00:43:22,309 - semantic_segmentation - Logging in file : ./logs/RandLANet_Toronto3D_torch/log_test_2022-06-15_00:43:22.txt\n",
      "INFO - 2022-06-15 00:43:22,311 - toronto3d - Found 1 pointclouds for test\n",
      "INFO - 2022-06-15 00:43:25,911 - semantic_segmentation - Initializing from scratch.\n",
      "INFO - 2022-06-15 00:43:25,912 - semantic_segmentation - Started testing\n",
      "test 0/1:  75%|████████████████████▉       | 3731827/4990714 [10:25<05:27, 3846.59it/s]"
     ]
    }
   ],
   "source": [
    "pipeline.run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3ebc7",
   "metadata": {},
   "source": [
    "## Running inference on individual point cloud\n",
    " \n",
    "`pipeline.run_inference` method lets us make predictions on novel point cloud data. It takes a dictionary with `point`, `feat` and `label` keys as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce851c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "train_split = dataset.get_split(\"test\")\n",
    "data = train_split.get_data(0)\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2653e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "results = pipeline.run_inference(data)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33b688",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "**Note:** You may replace `data` dictionary with custom point cloud data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e7a15",
   "metadata": {},
   "source": [
    "## Restoring from checkoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e916d4c",
   "metadata": {},
   "source": [
    "In the above example, you may notice that we are using latest trained model to make predictions. Latest trained models may not always be the best model so you may want to use model from previous checkpoints instead. \n",
    "\n",
    "`pipeline` provides `load_ckpt` method to restore model weights from training checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f017247",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"path/to/trained/model\"\n",
    "pipeline.load_ckpt(ckpt_path=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
