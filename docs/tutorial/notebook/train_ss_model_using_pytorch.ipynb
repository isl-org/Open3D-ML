{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc05fec",
   "metadata": {},
   "source": [
    "# Training a semantic segmentation model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25922b0",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to train a semantic segmentation model using PyTorch.\n",
    "\n",
    "Before you begin, ensure that you have *PyTorch* installed. To install a compatible version of PyTorch, use the requirement file:\n",
    "\n",
    "```sh\n",
    "pip install -r requirements-torch-cuda.txt\n",
    "```\n",
    "\n",
    "At a high level, we will:\n",
    "\n",
    "- Read a dataset and create a *'training'* split. For this example, we will use the `SemanticKITTI` dataset.\n",
    "- Train a model. We will train a `RandLANet` model on the *'training'* split.\n",
    "- Run a test on a *'test'* split to evaluate the model.\n",
    "- Run an inference on a custom point cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e89bac",
   "metadata": {},
   "source": [
    "## Reading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff079f",
   "metadata": {},
   "source": [
    "Downloading scripts are available in: `Open3D-ML/scripts/download_datasets`\n",
    "\n",
    "You can use any dataset available in the `ml3d.datasets` dataset namespace. Here, we will use the `SemanticKITTI` dataset and visualize it. You can use any of the other datasets to load data. However, you must understand that the parameters may vary for each dataset.\n",
    "\n",
    "We will read the dataset by specifying its path and then get all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508129cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# import torch\n",
    "import open3d.ml.torch as ml3d\n",
    "\n",
    "# Read a dataset by specifying the path. We are also providing the cache directory and training split.\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/',\n",
    "                                      cache_dir='./logs/cache',\n",
    "                                      training_split=['00'],\n",
    "                                      validation_split=['01'],\n",
    "                                      test_split=['01'])\n",
    "\n",
    "# Split the dataset for 'training'. You can get the other splits by passing 'validation' or 'test'\n",
    "train_split = dataset.get_split('training')\n",
    "\n",
    "#support of Open3d-ML visualizer in Jupyter Notebooks is in progress\n",
    "#view the frames using the visualizer\n",
    "#vis = ml3d.vis.Visualizer()\n",
    "#vis.visualize_dataset(dataset, 'training',indices=range(len(train_split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34dca8f",
   "metadata": {},
   "source": [
    "Now that you have visualized the dataset for training, let us train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc77ea",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "First, import the desired model from `open3d.ml.torch.models`.\n",
    "\n",
    "After you load a dataset, you can initialize any model and then train the model. The following example shows how you can train RandLANet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Semantic Segmentation Model using PyTorch\n",
    "\n",
    "# Import torch and the model to use for training\n",
    "import open3d.ml.torch as ml3d\n",
    "from open3d.ml.torch.models import RandLANet\n",
    "from open3d.ml.torch.pipelines import SemanticSegmentation\n",
    "\n",
    "# Read a dataset by specifying the path. We are also providing the cache directory and training split.\n",
    "# dataset = ml3d.datasets.SemanticKITTI(dataset_path='/Users/sanskara/data/SemanticKITTI/', cache_dir='./logs/cache',training_split=['00'], validation_split=['01'], test_split=['01'])\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/',\n",
    "                                      cache_dir='./logs/cache',\n",
    "                                      training_split=['00'],\n",
    "                                      validation_split=['01'],\n",
    "                                      test_split=['01'])\n",
    "\n",
    "# Initialize the RandLANet model.\n",
    "model = RandLANet(in_channels=3)\n",
    "pipeline = SemanticSegmentation(model=model,\n",
    "                                dataset=dataset,\n",
    "                                max_epoch=3,\n",
    "                                optimizer={'lr': 0.001},\n",
    "                                num_workers=0)\n",
    "\n",
    "# Run the training\n",
    "pipeline.run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b0bc9",
   "metadata": {},
   "source": [
    "The training checkpoints are saved in: `pipeline.main_log_dir` (default path is: “./logs/Model_Dataset/“). You can use them for testing and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fa94c",
   "metadata": {},
   "source": [
    "## Running a test\n",
    "\n",
    "Next, we will evaluate the trained model on the test split by calling the `run_test()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a385892",
   "metadata": {},
   "source": [
    "## Running an inference\n",
    "\n",
    "An inference processes point cloud and displays the results based on the trained model. For this example, we will use a trained `RandLANet` model.\n",
    "\n",
    "This example gets the pipeline, model, and dataset based on our previous training example. It runs the inference based the \"train\" split and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the SemanticKITTI dataset using the \"test\" split\n",
    "train_split = dataset.get_split(\"test\")\n",
    "data = train_split.get_data(0)\n",
    "\n",
    "# Run the inference\n",
    "results = pipeline.run_inference(data)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4039f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
