{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to read Open3D-ML datasets.\n",
    "\n",
    "You can use any dataset available in the `ml3d.datasets` namespace. For this example, we will use the `SemanticKITTI` dataset. You can use any of the other datasets to load data. However, you must understand that the parameters may vary for each dataset.\n",
    "\n",
    "To read a dataset in this example, we will supply the following parameter variables:\n",
    "\n",
    "- Dataset path (`dataset_path`)\n",
    "- Cache directory (`cache_dir`)\n",
    "- Dataset splits (for training, validation, and testing)\n",
    "\n",
    "> **For more theoretical background information on dataset splitting, please refer to these articles:**\n",
    ">\n",
    "> https://machinelearningcompass.com/dataset_optimization/split_data_the_right_way/\n",
    ">\n",
    "> https://www.freecodecamp.org/news/key-machine-learning-concepts-explained-dataset-splitting-and-random-forest/\n",
    "\n",
    "## Creating a global dataset object\n",
    "\n",
    "First, we import the Open3D-ML PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import open3d.ml.torch as ml3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `dataset` object, initializing it with *path, cache directory*, and *splits*. This `dataset` can read all the files inside `dataset_path` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a dataset by specifying the path. We are also providing the cache directory and splits.\n",
    "dataset = ml3d.datasets.SemanticKITTI(dataset_path='SemanticKITTI/',\n",
    "                                      cache_dir='./logs/cache',\n",
    "                                      training_split=['00'],\n",
    "                                      validation_split=['01'],\n",
    "                                      test_split=['01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of words regarding the *splits* variables: here, we isolate different portions of the `SemanticKITTI` dataset content and divide them into 3 different parts:\n",
    "\n",
    "1. `training_split` for data training. This part usually contains 70-75% of the global `dataset` content.\n",
    "2. `validation_split` for data validation. This part accounts for 10-15% of the global `dataset` content;\n",
    "3. `test_split` for testing. It contains test data and its size varies.\n",
    "\n",
    "Note the `SemanticKITTI` dataset folder structure:\n",
    "\n",
    "![dataset_structure](https://user-images.githubusercontent.com/93158890/162548755-28c541d3-3557-4903-a9a1-cc685d16dfc2.jpg)\n",
    "\n",
    "The three different *split* parameter variables instruct Open3D-ML subsystem to reference the following folder locations:\n",
    "\n",
    "- `training_split=['00']` points to `'SemanticKITTI/dataset/sequences/00/'`\n",
    "- `validation_split=['01']` points to `'SemanticKITTI/dataset/sequences/01/'`\n",
    "- `test_split=['01']` points to `'SemanticKITTI/dataset/sequences/01/'`\n",
    "\n",
    "> Note: **dataset split directories usually contain numerous point cloud files.** In our example we included only one point cloud file for extra speed and convenience.\n",
    "\n",
    "## Creating dataset split objects to query the data\n",
    "\n",
    "Next, we will create **dedicated** dataset split objects for specifying which split portion we would like to query.\n",
    "\n",
    "First, we create a `train_split` subset for training from the global `dataset` content we have initialized above using the `get_split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset for 'training'. You can get the other splits by passing 'validation' or 'test'\n",
    "train_split = dataset.get_split('training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, get validataion split.\n",
    "val_split = dataset.get_split('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a `test_split` subset for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test split\n",
    "test_split = dataset.get_split('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the size of dataset splits\n",
    "\n",
    "Let's see how large out *split* portions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length of splits\n",
    "print(len(train_split))\n",
    "print(len(val_split))\n",
    "print(len(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, Open3D-ML prints out the number of pointcloud files it found in `'SemanticKITTI/dataset/sequences/'`' `'/00/'` and `'/01/'` subdirectories we have specified earlier in `training_split=['00'], validation_split=['01'], test_split=['01']` varables for the `dataset`.\n",
    "\n",
    "## Querying dataset splits for data\n",
    "\n",
    "In this section, we are using the `train_split` dataset split object as an example. The procedure would be identical for all other dataset splits - `val_split` and `test_split`.\n",
    "\n",
    "In order to extract the data from the `train_split`, we can iterate through the `train_split` with the index `i` (ranging from `0` - `len(train_split)-1`) using the `get_data()` method.\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query splits for data, index should be from `0` to `len(split) - 1`\n",
    "for i in range(len(train_split)):\n",
    "    data = train_split.get_data(i)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` objects from the above `for` loop return a dictionary of points (`'point'`), features (`'feat'`), and labels (`'label'`), as we will see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_split.get_data(0)  # Dictionary of `point`, `feat`, and `label`\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **`'point'`** key value contains a set of 3D points/coordinates - X, Y, and Z:\n",
    "\n",
    "![dataset_coordinates](https://user-images.githubusercontent.com/93158890/162549410-6369cbd0-b835-4216-ba54-945e3f591395.jpg)\n",
    "\n",
    "- The **`'feat'`** (*features*) key value contains RGB color information for each of the above points.\n",
    "\n",
    "- The **`'label'`** key value represents which class the dataset content belongs to, i.e.: *pedestrian, vehicle, traffic light*, etc.\n",
    "\n",
    "### Querying dataset splits for attributes\n",
    "\n",
    "We can also extract corresponding point cloud information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = train_split.get_attr(0)\n",
    "print(\n",
    "    attr\n",
    ")  # Dictionary containing information about the data e.g. name, path, split, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atttributes returned are: `'idx'`(*index*), `'name'`, `'path'`, and `'split'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support of Open3d-ML visualizer in Jupyter Notebooks is in progress\n",
    "#view the frames using the visualizer\n",
    "#vis = ml3d.vis.Visualizer()\n",
    "#vis.visualize_dataset(dataset, 'training',indices=range(len(train_split)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
